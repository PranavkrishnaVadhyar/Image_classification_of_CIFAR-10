# -*- coding: utf-8 -*-
"""Copy of CIFAR10_VGG19_ fixed learning curves.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cUSuG4VnqANWXPsZuGn5XcTLJCXw6gq-

# Transfer Learning on CIFAR-10 Classification
It will be implemented in the following steps:-









1.   Downloading the CIFAR-10 dataset
2.   Importing the required libraries
3.   Preparing the dataset
4.   Defining the VGG transfer learning model
5.   Training the VGG model
6.   Making Predictions
7.  Evaluating performance
"""

#Keras library for CIFAR dataset

import tensorflow as tf

#Downloading the CIFAR dataset
(x_train,y_train),(x_test,y_test)=cifar10.load_data()

# Configure GPU settings
physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    print("GPU is available")
else:
    print("GPU is not available")

"""# Import the required libraries"""

#importing other required libraries
import numpy as np
import pandas as pd
from sklearn.utils.multiclass import unique_labels
import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns
import itertools
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from keras import Sequential
from keras.applications import VGG19 ##VGG19 and RsNet50 for Transfer Learning
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import SGD,Adam
from keras.callbacks import ReduceLROnPlateau
from keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout
from keras.utils import to_categorical

"""# Prepare the dataset"""

W_grid=5
L_grid=5
fig,axes = plt.subplots(L_grid,W_grid,figsize=(10,10))
axes=axes.ravel()
n_training=len(x_train)
for i in np.arange(0,L_grid * W_grid):
    index=np.random.randint(0,n_training)
    axes[i].imshow(x_train[index])
    axes[i].set_title(y_train[index])
    axes[i].axis('off')
plt.subplots_adjust(hspace=0.4)

#defining training and test sets
x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.3)

#Dimension of the dataset
print((x_train.shape,y_train.shape))
print((x_val.shape,y_val.shape))
print((x_test.shape,y_test.shape))

#Onehot Encoding the labels.
#Since we have 10 classes we should expect the shape[1] of y_train,y_val and y_test to change from 1 to 10
y_train=to_categorical(y_train)
y_val=to_categorical(y_val)
y_test=to_categorical(y_test)

# Normalize the pixel values to the range [0, 1]
x_train, x_val, x_test = x_train / 255.0, x_val / 255.0, x_test / 255.0

# Image Data Augmentation
train_generator = ImageDataGenerator(rotation_range=10,
                                     width_shift_range=0.1,
                                     height_shift_range=0.1,
                                     horizontal_flip=True)
val_generator = ImageDataGenerator()
test_generator = ImageDataGenerator()

#Fitting the augmentation defined above to the data
train_generator.fit(x_train)
val_generator.fit(x_val)
test_generator.fit(x_test)

"""# Define the model"""

# VGG19 Model
base_model = VGG19(include_top=False, weights='imagenet', input_shape=(32, 32, 3))

# Building the model
model = Sequential()
model.add(base_model)
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))

#Checking the final model summary
model.summary()

"""# Train the model"""

# Parameters
batch_size = 100
epochs = 20
learn_rate = 0.0001
adam = Adam(lr=learn_rate)

# Compiling the model
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

# Learning Rate Annealer
lrr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.01, patience=3, min_lr=1e-5)

# Training the model
history = model.fit(train_generator.flow(x_train, y_train, batch_size=batch_size),
                    epochs=epochs,
                    steps_per_epoch=x_train.shape[0] // batch_size,
                    validation_data=val_generator.flow(x_val, y_val, batch_size=batch_size),
                    validation_steps=x_val.shape[0] // batch_size,
                    callbacks=[lrr],
                    verbose=1)

# Plotting the training and validation loss and accuracy
f, ax = plt.subplots(2, 1, figsize=(12, 8))
ax[0].plot(history.history['loss'], color='b', label='Training Loss')
ax[0].plot(history.history['val_loss'], color='r', label='Validation Loss')
ax[0].legend()
ax[1].plot(history.history['accuracy'], color='b', label='Training Accuracy')
ax[1].plot(history.history['val_accuracy'], color='r', label='Validation Accuracy')
ax[1].legend()
plt.show()

loss , accuracy = model.evaluate(x_val,y_val)

print(loss , accuracy)

from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, auc, f1_score
from sklearn.preprocessing import label_binarize
from keras.utils import np_utils
from scipy import interp
from itertools import cycle

"""# **Model Evaluation**"""

# Getting the model's predictions
y_pred = model.predict(x_val)

# Class names for CIFAR-10 dataset
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

# Convert prediction results to one hot encoded form
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_val, axis=1)

# 1. Classification report
print("Classification Report:\n", classification_report(y_true, y_pred_classes, target_names=class_names))

# 2. Accuracy (You have already printed it but let's print it again for clarity)
print("Accuracy:", accuracy)

# 3. F1 score (It's a part of the classification report but can be isolated)
# The average parameter should be specified depending on the problem (macro/micro/weighted).
print("F1 Score:", f1_score(y_true, y_pred_classes, average='macro'))

# 4. ROC AUC Score (Multi-Class)
y_val_bin = label_binarize(y_true, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
n_classes = y_val_bin.shape[1]
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_val_bin[:, i], y_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
print("ROC AUC Score:", roc_auc_score(y_val, y_pred, multi_class='ovr'))

# 5. ROC AUC Curve (Multi-Class)
plt.figure(figsize=(12, 6))
colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'yellow', 'gray', 'brown', 'deeppink'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label='ROC curve of {0} (area = {1:0.2f})'
             ''.format(class_names[i], roc_auc[i]))
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC AUC Curve')
plt.legend(loc="lower right")
plt.show()

